{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU, LSTM, Bidirectional\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mae, categorical_accuracy\n",
    "from keras.models import load_model\n",
    "\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = []\n",
    "traindir = 'CWI 2018 Training Set/'\n",
    "for lang in os.listdir(traindir):\n",
    "    for filename in glob.iglob(traindir+lang+'/*_Train.tsv', recursive=True):\n",
    "        df = pd.read_csv(filename, header=None, sep='\\t')\n",
    "        df = df.rename(columns={0:'HIT_id', 1:'sentence', 2:'start', 3:'end',\n",
    "                               4:'target_word', 5:'native_all', 6:'non_native_all', \n",
    "                               7:'native_score', 8:'non_native_score',\n",
    "                               9:'binary_label', 10:'prob_label'})\n",
    "        df['filename'] = filename\n",
    "        df['lang'] = lang\n",
    "        train_dfs.append(df)\n",
    "        \n",
    "train_df = pd.concat(train_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target_word_lower'] = train_df['target_word'].astype(str).apply(str.lower)\n",
    "train_df['sentence_lower'] = train_df['sentence'].astype(str).apply(str.lower).apply(word_tokenize).apply(lambda x: ' '.join(x))\n",
    "\n",
    "train_df['doc'] =  train_df['target_word_lower'] + ' <s> '  + train_df['sentence_lower']\n",
    "train_df['lang_doc'] =  train_df['lang'] + ' <l> ' + train_df['doc']\n",
    "\n",
    "train_doc = train_df['lang_doc'].apply(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dfs = []\n",
    "traindir = 'CWI 2018 Training Set/'\n",
    "for lang in os.listdir(traindir):\n",
    "    for filename in glob.iglob(traindir+lang+'/*_Dev.tsv', recursive=True):\n",
    "        df = pd.read_csv(filename, header=None, sep='\\t')\n",
    "        df = df.rename(columns={0:'HIT_id', 1:'sentence', 2:'start', 3:'end',\n",
    "                               4:'target_word', 5:'native_all', 6:'non_native_all', \n",
    "                               7:'native_score', 8:'non_native_score',\n",
    "                               9:'binary_label', 10:'prob_label'})\n",
    "        df['filename'] = filename\n",
    "        df['lang'] = lang\n",
    "        dev_dfs.append(df)\n",
    "        \n",
    "dev_df = pd.concat(dev_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df['target_word_lower'] = dev_df['target_word'].astype(str).apply(str.lower)\n",
    "dev_df['sentence_lower'] = dev_df['sentence'].astype(str).apply(str.lower).apply(word_tokenize).apply(lambda x: ' '.join(x))\n",
    "\n",
    "dev_df['doc'] =  dev_df['target_word_lower'] + ' <s> '  + dev_df['sentence_lower']\n",
    "dev_df['lang_doc'] =  dev_df['lang'] + ' <l> ' + dev_df['doc']\n",
    "\n",
    "dev_doc = dev_df['lang_doc'].apply(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs = {}\n",
    "testdir = 'CWI 2018 Test Set/'\n",
    "for lang in os.listdir(testdir):\n",
    "    for filename in glob.iglob(testdir+lang+'/*.tsv', recursive=True):\n",
    "        df = pd.read_csv(filename, header=None, sep='\\t')\n",
    "        df = df.rename(columns={0:'HIT_id', 1:'sentence', 2:'start', 3:'end',\n",
    "                               4:'target_word', 5:'native_all', 6:'non_native_all', \n",
    "                               7:'native_score', 8:'non_native_score',\n",
    "                               9:'binary_label', 10:'prob_label'})\n",
    "        testset, _ = filename.split('_')\n",
    "        testset = lang + '_' + testset.split('/')[-1]\n",
    "        df['filename'] = filename\n",
    "        df['lang'] = lang\n",
    "        test_dfs[testset] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['german_German', 'spanish_Spanish', 'english_WikiNews', 'french_French', 'english_Wikipedia', 'english_News'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = {}\n",
    "for lang in test_dfs.keys():\n",
    "    test_dfs[lang]['target_word_lower'] = test_dfs[lang]['target_word'].astype(str).apply(str.lower)\n",
    "    test_dfs[lang]['sentence_lower'] = test_dfs[lang]['sentence'].astype(str).apply(str.lower).apply(word_tokenize).apply(lambda x: ' '.join(x))\n",
    "    test_dfs[lang]['doc'] =  test_dfs[lang]['target_word_lower'] + ' <s> '  + test_dfs[lang]['sentence_lower']\n",
    "    test_dfs[lang]['lang_doc'] =  test_dfs[lang]['lang'] + ' <l> ' + test_dfs[lang]['doc']\n",
    "    test_docs[lang] = test_dfs[lang]['lang_doc'].apply(str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Dictionary(train_doc)\n",
    "\n",
    "def vectorize_sent(sent):\n",
    "    return vocab.doc2idx(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 150\n",
    "X_train = sequence.pad_sequences(train_doc.apply(vectorize_sent), maxlen=max_length)\n",
    "y_train = train_df['binary_label']\n",
    "\n",
    "X_dev = sequence.pad_sequences(dev_doc.apply(vectorize_sent), maxlen=max_length)\n",
    "y_dev = dev_df['binary_label']\n",
    "\n",
    "X_test_en_news = sequence.pad_sequences(test_docs['english_News'].apply(vectorize_sent), maxlen=max_length)\n",
    "X_test_en_wikinews = sequence.pad_sequences(test_docs['english_WikiNews'].apply(vectorize_sent), maxlen=max_length)\n",
    "X_test_en_wiki = sequence.pad_sequences(test_docs['english_Wikipedia'].apply(vectorize_sent), maxlen=max_length)\n",
    "\n",
    "X_test_es = sequence.pad_sequences(test_docs['spanish_Spanish'].apply(vectorize_sent), maxlen=max_length)\n",
    "X_test_de = sequence.pad_sequences(test_docs['german_German'].apply(vectorize_sent), maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 100)          1899900   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 150, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 75, 32)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 75, 200)           106400    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,136,433\n",
      "Trainable params: 2,136,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(5)\n",
    "embedding_vecor_length = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab.keys()), embedding_vecor_length, input_length=150))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(Bidirectional(LSTM(100, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"models06/{epoch:02d}-{acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47200/47200 [==============================] - 18s 373us/step - loss: 0.6801 - acc: 0.5904\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.59038, saving model to models06/01-0.5904.hdf5\n",
      "Epoch 2/20\n",
      "47200/47200 [==============================] - 13s 270us/step - loss: 0.6715 - acc: 0.5917\n",
      "\n",
      "Epoch 00002: acc improved from 0.59038 to 0.59174, saving model to models06/02-0.5917.hdf5\n",
      "Epoch 3/20\n",
      "47200/47200 [==============================] - 13s 272us/step - loss: 0.6562 - acc: 0.6059\n",
      "\n",
      "Epoch 00003: acc improved from 0.59174 to 0.60587, saving model to models06/03-0.6059.hdf5\n",
      "Epoch 4/20\n",
      "47200/47200 [==============================] - 13s 275us/step - loss: 0.6384 - acc: 0.6390\n",
      "\n",
      "Epoch 00004: acc improved from 0.60587 to 0.63903, saving model to models06/04-0.6390.hdf5\n",
      "Epoch 5/20\n",
      "47200/47200 [==============================] - 13s 282us/step - loss: 0.6203 - acc: 0.6589\n",
      "\n",
      "Epoch 00005: acc improved from 0.63903 to 0.65890, saving model to models06/05-0.6589.hdf5\n",
      "Epoch 6/20\n",
      "47200/47200 [==============================] - 13s 282us/step - loss: 0.5534 - acc: 0.7257\n",
      "\n",
      "Epoch 00006: acc improved from 0.65890 to 0.72568, saving model to models06/06-0.7257.hdf5\n",
      "Epoch 7/20\n",
      "47200/47200 [==============================] - 13s 281us/step - loss: 0.4184 - acc: 0.8248\n",
      "\n",
      "Epoch 00007: acc improved from 0.72568 to 0.82481, saving model to models06/07-0.8248.hdf5\n",
      "Epoch 8/20\n",
      "47200/47200 [==============================] - 13s 280us/step - loss: 0.3245 - acc: 0.8768\n",
      "\n",
      "Epoch 00008: acc improved from 0.82481 to 0.87684, saving model to models06/08-0.8768.hdf5\n",
      "Epoch 9/20\n",
      "47200/47200 [==============================] - 13s 278us/step - loss: 0.2808 - acc: 0.8963\n",
      "\n",
      "Epoch 00009: acc improved from 0.87684 to 0.89629, saving model to models06/09-0.8963.hdf5\n",
      "Epoch 10/20\n",
      "47200/47200 [==============================] - 13s 267us/step - loss: 0.2506 - acc: 0.9082\n",
      "\n",
      "Epoch 00010: acc improved from 0.89629 to 0.90820, saving model to models06/10-0.9082.hdf5\n",
      "Epoch 11/20\n",
      "47200/47200 [==============================] - 13s 265us/step - loss: 0.2304 - acc: 0.9142\n",
      "\n",
      "Epoch 00011: acc improved from 0.90820 to 0.91417, saving model to models06/11-0.9142.hdf5\n",
      "Epoch 12/20\n",
      "47200/47200 [==============================] - 12s 264us/step - loss: 0.2124 - acc: 0.9199\n",
      "\n",
      "Epoch 00012: acc improved from 0.91417 to 0.91992, saving model to models06/12-0.9199.hdf5\n",
      "Epoch 13/20\n",
      "47200/47200 [==============================] - 13s 265us/step - loss: 0.1982 - acc: 0.9244\n",
      "\n",
      "Epoch 00013: acc improved from 0.91992 to 0.92436, saving model to models06/13-0.9244.hdf5\n",
      "Epoch 14/20\n",
      "47200/47200 [==============================] - 13s 267us/step - loss: 0.1875 - acc: 0.9293\n",
      "\n",
      "Epoch 00014: acc improved from 0.92436 to 0.92926, saving model to models06/14-0.9293.hdf5\n",
      "Epoch 15/20\n",
      "47200/47200 [==============================] - 13s 265us/step - loss: 0.1765 - acc: 0.9324\n",
      "\n",
      "Epoch 00015: acc improved from 0.92926 to 0.93239, saving model to models06/15-0.9324.hdf5\n",
      "Epoch 16/20\n",
      "47200/47200 [==============================] - 12s 265us/step - loss: 0.1697 - acc: 0.9352\n",
      "\n",
      "Epoch 00016: acc improved from 0.93239 to 0.93517, saving model to models06/16-0.9352.hdf5\n",
      "Epoch 17/20\n",
      "47200/47200 [==============================] - 13s 266us/step - loss: 0.1593 - acc: 0.9386\n",
      "\n",
      "Epoch 00017: acc improved from 0.93517 to 0.93862, saving model to models06/17-0.9386.hdf5\n",
      "Epoch 18/20\n",
      "47200/47200 [==============================] - 12s 263us/step - loss: 0.1539 - acc: 0.9407\n",
      "\n",
      "Epoch 00018: acc improved from 0.93862 to 0.94068, saving model to models06/18-0.9407.hdf5\n",
      "Epoch 19/20\n",
      "47200/47200 [==============================] - 13s 265us/step - loss: 0.1470 - acc: 0.9429\n",
      "\n",
      "Epoch 00019: acc improved from 0.94068 to 0.94290, saving model to models06/19-0.9429.hdf5\n",
      "Epoch 20/20\n",
      "47200/47200 [==============================] - 13s 265us/step - loss: 0.1401 - acc: 0.9454\n",
      "\n",
      "Epoch 00020: acc improved from 0.94290 to 0.94536, saving model to models06/20-0.9454.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cdc523fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=3000, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "20-0.9296.hdf5 0.7361183639813549\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "19-0.9263.hdf5 0.7369886858345012\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "18-0.9262.hdf5 0.7352480419622077\n",
      "5745/5745 [==============================] - 7s 1ms/step\n",
      "17-0.9232.hdf5 0.7397737162957718\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "16-0.9217.hdf5 0.7382071366613071\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "15-0.9190.hdf5 0.7361183638153541\n",
      "5745/5745 [==============================] - 7s 1ms/step\n",
      "14-0.9163.hdf5 0.7389033942766248\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "13-0.9131.hdf5 0.7402959095072601\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "12-0.9111.hdf5 0.7347258487714696\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "11-0.9038.hdf5 0.7389033942766248\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "10-0.8976.hdf5 0.7378590080403992\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "09-0.8760.hdf5 0.7342036555599813\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "08-0.8251.hdf5 0.7258485641761694\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "07-0.7386.hdf5 0.6901653612251382\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "06-0.6734.hdf5 0.6436901654026839\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "05-0.6442.hdf5 0.5987815492146943\n",
      "5745/5745 [==============================] - 9s 2ms/step\n",
      "04-0.6271.hdf5 0.5906005222347117\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "03-0.5939.hdf5 0.5911227154461999\n",
      "5745/5745 [==============================] - 8s 1ms/step\n",
      "02-0.5917.hdf5 0.5865970409466351\n",
      "5745/5745 [==============================] - 9s 1ms/step\n",
      "01-0.5907.hdf5 0.5865970409466351\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for modelfile in reversed(sorted(os.listdir('models05/'))):\n",
    "    model = load_model('models05/' + modelfile)\n",
    "    _, score = model.evaluate(X_dev, y_dev, verbose=1)\n",
    "    scores.append((score, modelfile))\n",
    "    print(modelfile, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = '11-0.9038.hdf5'\n",
    "model = load_model('models05/' + modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('puddlepod.en_news.05.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_en_news):\n",
    "        print(int(pred[0] > 0.5), end='\\n', file=fout)\n",
    "        \n",
    "with open('puddlepod.en_wikinews.05.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_en_wikinews):\n",
    "        print(int(pred[0] > 0.5), end='\\n', file=fout)\n",
    "        \n",
    "with open('puddlepod.en_wiki.05.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_en_wiki):\n",
    "        print(int(pred[0] > 0.5), end='\\n', file=fout)\n",
    "\n",
    "with open('puddlepod.es.05.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_es):\n",
    "        print(int(pred[0] > 0.5), end='\\n', file=fout)\n",
    "        \n",
    "with open('puddlepod.de.05.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_de):\n",
    "        print(int(pred[0] > 0.5), end='\\n', file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prob_puddlepod.en_news.04.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_en_news):\n",
    "        print(\"{0:.2f}\".format(pred[0]), end='\\n', file=fout)\n",
    "        \n",
    "with open('prob_puddlepod.en_wikinews.04.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_en_wikinews):\n",
    "        print(\"{0:.2f}\".format(pred[0]) , end='\\n', file=fout)\n",
    "        \n",
    "with open('prob_puddlepod.en_wiki.04.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_en_wiki):\n",
    "        print(\"{0:.2f}\".format(pred[0]) , end='\\n', file=fout)\n",
    "        \n",
    "with open('prob_puddlepod.es.04.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_es):\n",
    "        print(\"{0:.2f}\".format(pred[0]), end='\\n', file=fout)\n",
    "        \n",
    "with open('prob_puddlepod.de.04.tsv', 'w') as fout:\n",
    "    for pred in model.predict(X_test_de):\n",
    "        print(\"{0:.2f}\".format(pred[0]), end='\\n', file=fout)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
